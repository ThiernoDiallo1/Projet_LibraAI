# üöÄ D√©marrage rapide LibraAi

## ‚ö° √âtapes pour d√©marrer l'application

### 1. Installer les d√©pendances Backend

```bash
cd backend
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate

pip install -r requirements.txt
```

### 2. Installer les d√©pendances Frontend

```bash
cd frontend
npm install
```

### 3. D√©marrer le Backend (Terminal 1)

```bash
cd backend
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/macOS
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**‚úÖ Le backend doit afficher :**

```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

### 4. D√©marrer le Frontend (Terminal 2)

```bash
cd frontend
npm run dev
```

**‚úÖ Le frontend doit afficher :**

```
  VITE v4.4.5  ready in 500 ms

  ‚ûú  Local:   http://localhost:3000/
  ‚ûú  Network: use --host to expose
```

### 5. Acc√©der √† l'application

- **Application** : http://localhost:3000
- **API Backend** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs

---

## üîß R√©solution des probl√®mes courants

### ‚ùå Erreur "ERR_CONNECTION_REFUSED"

**Cause** : Le backend n'est pas d√©marr√©

**Solution** :

1. Ouvrir un terminal dans le dossier `backend`
2. Activer l'environnement virtuel
3. D√©marrer le serveur avec `uvicorn main:app --reload`

### ‚ùå Erreur "Module not found"

**Cause** : D√©pendances non install√©es

**Solution** :

```bash
# Backend
cd backend
pip install -r requirements.txt

# Frontend
cd frontend
npm install
```

### ‚ùå Erreur MongoDB

**Cause** : Probl√®me de connexion √† la base de donn√©es

**Solution** :

1. V√©rifier que MongoDB Atlas est accessible
2. V√©rifier la cha√Æne de connexion dans `.env`
3. V√©rifier que l'IP est autoris√©e (0.0.0.0/0)

### ‚ùå Erreur Ollama

**Cause** : Ollama n'est pas install√© ou d√©marr√©

**Solution** :

1. Installer Ollama : https://ollama.ai/
2. T√©l√©charger le mod√®le : `ollama pull llama2`
3. V√©rifier : `ollama list`

---

## üìù Ordre de d√©marrage recommand√©

1. **Backend d'abord** (port 8000)
2. **Frontend ensuite** (port 3000)
3. **V√©rifier les deux** fonctionnent avant de tester

---

## üéØ Test rapide

1. Aller sur http://localhost:3000
2. Cliquer sur "Inscription"
3. Cr√©er un compte
4. Se connecter
5. Explorer l'application

---

## üìû Si √ßa ne marche toujours pas

1. V√©rifier que les deux terminaux sont ouverts
2. V√©rifier les messages d'erreur dans les terminaux
3. V√©rifier que les ports 3000 et 8000 ne sont pas utilis√©s
4. Red√©marrer les serveurs si n√©cessaire
